{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:03.078377Z",
     "iopub.status.busy": "2025-10-15T18:14:03.077522Z",
     "iopub.status.idle": "2025-10-15T18:14:03.082921Z",
     "shell.execute_reply": "2025-10-15T18:14:03.082044Z",
     "shell.execute_reply.started": "2025-10-15T18:14:03.078353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'using_pyarrow_string_dtype' from 'pandas._config' (d:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\_config\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#import kagglehub\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\__init__.py:62\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     64\u001b[39m     ArrowDtype,\n\u001b[32m     65\u001b[39m     Int8Dtype,\n\u001b[32m     66\u001b[39m     Int16Dtype,\n\u001b[32m     67\u001b[39m     Int32Dtype,\n\u001b[32m     68\u001b[39m     Int64Dtype,\n\u001b[32m     69\u001b[39m     UInt8Dtype,\n\u001b[32m     70\u001b[39m     UInt16Dtype,\n\u001b[32m     71\u001b[39m     UInt32Dtype,\n\u001b[32m     72\u001b[39m     UInt64Dtype,\n\u001b[32m     73\u001b[39m     Float32Dtype,\n\u001b[32m     74\u001b[39m     Float64Dtype,\n\u001b[32m     75\u001b[39m     CategoricalDtype,\n\u001b[32m     76\u001b[39m     PeriodDtype,\n\u001b[32m     77\u001b[39m     IntervalDtype,\n\u001b[32m     78\u001b[39m     DatetimeTZDtype,\n\u001b[32m     79\u001b[39m     StringDtype,\n\u001b[32m     80\u001b[39m     BooleanDtype,\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     82\u001b[39m     NA,\n\u001b[32m     83\u001b[39m     isna,\n\u001b[32m     84\u001b[39m     isnull,\n\u001b[32m     85\u001b[39m     notna,\n\u001b[32m     86\u001b[39m     notnull,\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     88\u001b[39m     Index,\n\u001b[32m     89\u001b[39m     CategoricalIndex,\n\u001b[32m     90\u001b[39m     RangeIndex,\n\u001b[32m     91\u001b[39m     MultiIndex,\n\u001b[32m     92\u001b[39m     IntervalIndex,\n\u001b[32m     93\u001b[39m     TimedeltaIndex,\n\u001b[32m     94\u001b[39m     DatetimeIndex,\n\u001b[32m     95\u001b[39m     PeriodIndex,\n\u001b[32m     96\u001b[39m     IndexSlice,\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     98\u001b[39m     NaT,\n\u001b[32m     99\u001b[39m     Period,\n\u001b[32m    100\u001b[39m     period_range,\n\u001b[32m    101\u001b[39m     Timedelta,\n\u001b[32m    102\u001b[39m     timedelta_range,\n\u001b[32m    103\u001b[39m     Timestamp,\n\u001b[32m    104\u001b[39m     date_range,\n\u001b[32m    105\u001b[39m     bdate_range,\n\u001b[32m    106\u001b[39m     Interval,\n\u001b[32m    107\u001b[39m     interval_range,\n\u001b[32m    108\u001b[39m     DateOffset,\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    110\u001b[39m     to_numeric,\n\u001b[32m    111\u001b[39m     to_datetime,\n\u001b[32m    112\u001b[39m     to_timedelta,\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    114\u001b[39m     Flags,\n\u001b[32m    115\u001b[39m     Grouper,\n\u001b[32m    116\u001b[39m     factorize,\n\u001b[32m    117\u001b[39m     unique,\n\u001b[32m    118\u001b[39m     value_counts,\n\u001b[32m    119\u001b[39m     NamedAgg,\n\u001b[32m    120\u001b[39m     array,\n\u001b[32m    121\u001b[39m     Categorical,\n\u001b[32m    122\u001b[39m     set_eng_float_format,\n\u001b[32m    123\u001b[39m     Series,\n\u001b[32m    124\u001b[39m     DataFrame,\n\u001b[32m    125\u001b[39m )\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\api.py:47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     Grouper,\n\u001b[32m     49\u001b[39m     NamedAgg,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     CategoricalIndex,\n\u001b[32m     53\u001b[39m     DatetimeIndex,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     TimedeltaIndex,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatetimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     bdate_range,\n\u001b[32m     63\u001b[39m     date_range,\n\u001b[32m     64\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\groupby\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     DataFrameGroupBy,\n\u001b[32m      3\u001b[39m     NamedAgg,\n\u001b[32m      4\u001b[39m     SeriesGroupBy,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:68\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     61\u001b[39m     GroupByApply,\n\u001b[32m     62\u001b[39m     maybe_mangle_lambdas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     warn_alias_replacement,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcom\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     70\u001b[39m     base,\n\u001b[32m     71\u001b[39m     ops,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     GroupBy,\n\u001b[32m     75\u001b[39m     GroupByPlot,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     _transform_template,\n\u001b[32m     80\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\frame.py:149\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseFrameAccessor\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    145\u001b[39m     ensure_wrapped_if_datetimelike,\n\u001b[32m    146\u001b[39m     sanitize_array,\n\u001b[32m    147\u001b[39m     sanitize_masked_array,\n\u001b[32m    148\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    150\u001b[39m     NDFrame,\n\u001b[32m    151\u001b[39m     make_doc,\n\u001b[32m    152\u001b[39m )\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_key_length\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    155\u001b[39m     DatetimeIndex,\n\u001b[32m    156\u001b[39m     Index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m     ensure_index_from_sequences,\n\u001b[32m    161\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\generic.py:152\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    144\u001b[39m     is_hashable,\n\u001b[32m    145\u001b[39m     is_nested_list_like,\n\u001b[32m    146\u001b[39m )\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    148\u001b[39m     isna,\n\u001b[32m    149\u001b[39m     notna,\n\u001b[32m    150\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    153\u001b[39m     algorithms \u001b[38;5;28;01mas\u001b[39;00m algos,\n\u001b[32m    154\u001b[39m     arraylike,\n\u001b[32m    155\u001b[39m     common,\n\u001b[32m    156\u001b[39m     indexing,\n\u001b[32m    157\u001b[39m     missing,\n\u001b[32m    158\u001b[39m     nanops,\n\u001b[32m    159\u001b[39m     sample,\n\u001b[32m    160\u001b[39m )\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_algos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreplace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m should_use_regex\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExtensionArray\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\indexing.py:79\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     70\u001b[39m     array \u001b[38;5;28;01mas\u001b[39;00m pd_array,\n\u001b[32m     71\u001b[39m     extract_array,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     check_array_indexer,\n\u001b[32m     75\u001b[39m     is_list_like_indexer,\n\u001b[32m     76\u001b[39m     is_scalar_indexer,\n\u001b[32m     77\u001b[39m     length_of_indexer,\n\u001b[32m     78\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     80\u001b[39m     Index,\n\u001b[32m     81\u001b[39m     MultiIndex,\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     86\u001b[39m         Hashable,\n\u001b[32m     87\u001b[39m         Sequence,\n\u001b[32m     88\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\indexes\\api.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_common_type\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m safe_sort\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     Index,\n\u001b[32m     22\u001b[39m     _new_Index,\n\u001b[32m     23\u001b[39m     ensure_index,\n\u001b[32m     24\u001b[39m     ensure_index_from_sequences,\n\u001b[32m     25\u001b[39m     get_unanimous_names,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcategory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CategoricalIndex\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatetimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatetimeIndex\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:23\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     get_option,\n\u001b[32m     25\u001b[39m     using_copy_on_write,\n\u001b[32m     26\u001b[39m     using_pyarrow_string_dtype,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     NaT,\n\u001b[32m     31\u001b[39m     algos \u001b[38;5;28;01mas\u001b[39;00m libalgos,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     writers,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockValuesRefs\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'using_pyarrow_string_dtype' from 'pandas._config' (d:\\Anaconda\\envs\\genai\\Lib\\site-packages\\pandas\\_config\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:03.103347Z",
     "iopub.status.busy": "2025-10-15T18:14:03.102814Z",
     "iopub.status.idle": "2025-10-15T18:14:04.167297Z",
     "shell.execute_reply": "2025-10-15T18:14:04.166534Z",
     "shell.execute_reply.started": "2025-10-15T18:14:03.103322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/empathetic-dialogues-facebook-ai\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"atharvjairath/empathetic-dialogues-facebook-ai\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:04.168852Z",
     "iopub.status.busy": "2025-10-15T18:14:04.168546Z",
     "iopub.status.idle": "2025-10-15T18:14:04.547298Z",
     "shell.execute_reply": "2025-10-15T18:14:04.546480Z",
     "shell.execute_reply.started": "2025-10-15T18:14:04.168833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Total rows: 64636\n",
      "Training set size: 51708\n",
      "Validation set size: 6464\n",
      "Test set size: 6464\n"
     ]
    }
   ],
   "source": [
    "# The path provided by kagglehub\n",
    "dataset_path = \"/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"Dataset loaded successfully. Total rows: {len(df)}\")\n",
    "\n",
    "# First, split into 80% train and 20% temporary (for val + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Next, split the 20% temporary set in half to get 10% validation and 10% test\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:04.548663Z",
     "iopub.status.busy": "2025-10-15T18:14:04.548383Z",
     "iopub.status.idle": "2025-10-15T18:14:04.553170Z",
     "shell.execute_reply": "2025-10-15T18:14:04.552331Z",
     "shell.execute_reply.started": "2025-10-15T18:14:04.548645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Rule 1: Remove \"customer :\" or \"agent :\" from the BEGINNING of the string\n",
    "    text = re.sub(r'^(customer|agent)\\s*:\\s*', '', text, flags=re.I).strip()\n",
    "\n",
    "    # Rule 2: Remove \"agent :\" from the END of the string\n",
    "    text = re.sub(r'agent\\s*:\\s*$', '', text, flags=re.I).strip()\n",
    "\n",
    "    # Rule 3: Handle punctuation and whitespace\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:04.555320Z",
     "iopub.status.busy": "2025-10-15T18:14:04.555016Z",
     "iopub.status.idle": "2025-10-15T18:14:06.739533Z",
     "shell.execute_reply": "2025-10-15T18:14:06.738928Z",
     "shell.execute_reply.started": "2025-10-15T18:14:04.555292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the correct columns in all three data splits\n",
    "for df_split in [train_df, val_df, test_df]:\n",
    "    df_split['Situation_normalized'] = df_split['Situation'].apply(normalize_text)\n",
    "    df_split['empathetic_dialogues_normalized'] = df_split['empathetic_dialogues'].apply(normalize_text)\n",
    "    df_split['labels_normalized'] = df_split['labels'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:06.740400Z",
     "iopub.status.busy": "2025-10-15T18:14:06.740188Z",
     "iopub.status.idle": "2025-10-15T18:14:06.760721Z",
     "shell.execute_reply": "2025-10-15T18:14:06.759954Z",
     "shell.execute_reply.started": "2025-10-15T18:14:06.740384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a list of all text from the training set to build the vocabulary\n",
    "corpus = list(train_df['empathetic_dialogues_normalized']) + \\\n",
    "         list(train_df['Situation_normalized']) + \\\n",
    "         list(train_df['labels_normalized'])\n",
    "\n",
    "# We need an iterator for the tokenizer training\n",
    "def corpus_iterator():\n",
    "    for text in corpus:\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:06.761595Z",
     "iopub.status.busy": "2025-10-15T18:14:06.761404Z",
     "iopub.status.idle": "2025-10-15T18:14:07.911702Z",
     "shell.execute_reply": "2025-10-15T18:14:07.910915Z",
     "shell.execute_reply.started": "2025-10-15T18:14:06.761574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19342\n",
      "Tokenizer saved to /kaggle/working/my_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Define the trainer\n",
    "trainer = WordLevelTrainer(special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "\n",
    "# Train the tokenizer on your training data\n",
    "tokenizer.train_from_iterator(corpus_iterator(), trainer=trainer)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Save the tokenizer for later use \n",
    "save_path = \"/kaggle/working/my_tokenizer.json\"\n",
    "tokenizer.save(save_path)\n",
    "print(f\"Tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:07.912630Z",
     "iopub.status.busy": "2025-10-15T18:14:07.912432Z",
     "iopub.status.idle": "2025-10-15T18:14:07.917030Z",
     "shell.execute_reply": "2025-10-15T18:14:07.916325Z",
     "shell.execute_reply.started": "2025-10-15T18:14:07.912614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token IDs:\n",
      "<unk> ID: 3\n",
      "<pad> ID: 0\n",
      "<bos> ID: 1\n",
      "<eos> ID: 2\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\n",
    "\n",
    "print(\"Special token IDs:\")\n",
    "print(f\"<unk> ID: {tokenizer.token_to_id('<unk>')}\")\n",
    "print(f\"<pad> ID: {tokenizer.token_to_id('<pad>')}\")\n",
    "print(f\"<bos> ID: {tokenizer.token_to_id('<bos>')}\")\n",
    "print(f\"<eos> ID: {tokenizer.token_to_id('<eos>')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:07.917943Z",
     "iopub.status.busy": "2025-10-15T18:14:07.917724Z",
     "iopub.status.idle": "2025-10-15T18:14:07.931474Z",
     "shell.execute_reply": "2025-10-15T18:14:07.930855Z",
     "shell.execute_reply.started": "2025-10-15T18:14:07.917919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_input_string(row):\n",
    "    emotion = row['emotion']\n",
    "    situation = row['Situation_normalized']\n",
    "    customer_utterance = row['empathetic_dialogues_normalized']\n",
    "    \n",
    "    # Format the string exactly as specified\n",
    "    input_str = f\"Emotion: {emotion} | Situation: {situation} | Customer: {customer_utterance} Agent:\"\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:07.932410Z",
     "iopub.status.busy": "2025-10-15T18:14:07.932155Z",
     "iopub.status.idle": "2025-10-15T18:14:08.461024Z",
     "shell.execute_reply": "2025-10-15T18:14:08.460392Z",
     "shell.execute_reply.started": "2025-10-15T18:14:07.932394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y columns created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to create the 'X' and 'Y' columns for all data splits\n",
    "for df_split in [train_df, val_df, test_df]:\n",
    "    df_split['X'] = df_split.apply(create_input_string, axis=1)\n",
    "    # The target 'Y' is the normalized agent's reply from the 'labels' column\n",
    "    df_split['Y'] = df_split['labels_normalized']\n",
    "\n",
    "print(\"X and Y columns created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.463206Z",
     "iopub.status.busy": "2025-10-15T18:14:08.463007Z",
     "iopub.status.idle": "2025-10-15T18:14:08.467336Z",
     "shell.execute_reply": "2025-10-15T18:14:08.466474Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.463191Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example ---\n",
      "INPUT (X):\n",
      "Emotion: nostalgic | Situation: i had to go buy legos for my nephew the other day . makes me miss the days when my girls were young enough to play with them . | Customer: were you embarrassed or what happend ? Agent:\n",
      "\n",
      "TARGET (Y):\n",
      "no just this feeling overcame me that my kids just have outgrown this time .\n"
     ]
    }
   ],
   "source": [
    "# Inspect a final example from the training set to verify\n",
    "print(\"\\n--- Example ---\")\n",
    "print(\"INPUT (X):\")\n",
    "print(train_df['X'].iloc[0])\n",
    "print(\"\\nTARGET (Y):\")\n",
    "print(train_df['Y'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.468134Z",
     "iopub.status.busy": "2025-10-15T18:14:08.467918Z",
     "iopub.status.idle": "2025-10-15T18:14:08.485951Z",
     "shell.execute_reply": "2025-10-15T18:14:08.485314Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.468114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# --- MODEL ARCHITECTURE ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers_encoder: int, nlayers_decoder: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=nlayers_encoder,\n",
    "                                          num_decoder_layers=nlayers_decoder,\n",
    "                                          dim_feedforward=d_hid, dropout=dropout,\n",
    "                                          batch_first=False)\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_padding_mask: torch.Tensor,\n",
    "                tgt_padding_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        \n",
    "        output = self.transformer(src_emb, tgt_emb,\n",
    "                                  src_key_padding_mask=src_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                  memory_key_padding_mask=src_padding_mask,\n",
    "                                  tgt_mask=tgt_mask)\n",
    "        \n",
    "        return self.generator(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.486795Z",
     "iopub.status.busy": "2025-10-15T18:14:08.486568Z",
     "iopub.status.idle": "2025-10-15T18:14:08.505509Z",
     "shell.execute_reply": "2025-10-15T18:14:08.504692Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.486776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # --- THIS IS THE UPDATED PART ---\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # THIS IS THE FIX:\n",
    "            # The mask has shape (seq_len, seq_len)\n",
    "            # We add 2 dimensions to make it (1, 1, seq_len, seq_len)\n",
    "            # so it can broadcast to the scores' shape of (batch, h, seq_len, seq_len)\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), -1e9)\n",
    "            \n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "            \n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        query = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        key = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        value = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_scores = self.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.506521Z",
     "iopub.status.busy": "2025-10-15T18:14:08.506263Z",
     "iopub.status.idle": "2025-10-15T18:14:08.524044Z",
     "shell.execute_reply": "2025-10-15T18:14:08.523455Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.506500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The dimension of the embedding.\n",
    "            h (int): The number of attention heads.\n",
    "            d_ff (int): The dimension of the feed-forward network.\n",
    "            dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor from the previous layer.\n",
    "            mask (torch.Tensor): The mask for the input sequence.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of the encoder layer.\n",
    "        \"\"\"\n",
    "        # --- First sub-layer: Multi-Head Self-Attention ---\n",
    "        # The query, key, and value are all the same: the input 'x'. This is \"self-attention\".\n",
    "        attn_output = self.self_attn(q=x, k=x, v=x, mask=mask)\n",
    "        \n",
    "        # Apply the first residual connection (\"Add\") and Layer Normalization (\"Norm\")\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # --- Second sub-layer: Feed-Forward Network ---\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Apply the second residual connection and Layer Normalization\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.524978Z",
     "iopub.status.busy": "2025-10-15T18:14:08.524728Z",
     "iopub.status.idle": "2025-10-15T18:14:08.542046Z",
     "shell.execute_reply": "2025-10-15T18:14:08.541298Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.524958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The dimension of the embedding.\n",
    "            h (int): The number of attention heads.\n",
    "            d_ff (int): The dimension of the feed-forward network.\n",
    "            dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, \n",
    "                source_mask: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): The input from the previous decoder layer.\n",
    "            encoder_output (torch.Tensor): The final output of the encoder stack.\n",
    "            source_mask (torch.Tensor): The mask for the encoder output.\n",
    "            target_mask (torch.Tensor): The mask for the decoder input.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of the decoder layer.\n",
    "        \"\"\"\n",
    "        # --- First sub-layer: Masked Multi-Head Self-Attention ---\n",
    "        attn_output = self.self_attn(q=x, k=x, v=x, mask=target_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # --- Second sub-layer: Encoder-Decoder Cross-Attention ---\n",
    "        # Query comes from the decoder, Key and Value come from the encoder.\n",
    "        attn_output = self.cross_attn(q=x, k=encoder_output, v=encoder_output, mask=source_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        # --- Third sub-layer: Feed-Forward Network ---\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.543131Z",
     "iopub.status.busy": "2025-10-15T18:14:08.542809Z",
     "iopub.status.idle": "2025-10-15T18:14:08.560318Z",
     "shell.execute_reply": "2025-10-15T18:14:08.559743Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.543107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replace your old Transformer class with this one\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_encoder_layers: int, \n",
    "                 num_decoder_layers: int, num_heads: int, d_ff: int, \n",
    "                 dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)])\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # --- THIS FORWARD METHOD IS UPDATED ---\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # --- NEW: Generate target mask internally ---\n",
    "        tgt_seq_len = tgt.shape[0]\n",
    "        tgt_device = tgt.device\n",
    "        tgt_causal_mask = torch.triu(torch.ones((tgt_seq_len, tgt_seq_len), device=tgt_device), diagonal=1).bool()\n",
    "        \n",
    "        # Embed and add positional encoding\n",
    "        src_embedded = self.pos_encoder(self.embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_embedded = self.pos_encoder(self.embedding(tgt) * math.sqrt(self.d_model))\n",
    "\n",
    "        # Encoder Pass\n",
    "        encoder_output = src_embedded\n",
    "        for layer in self.encoder:\n",
    "            encoder_output = layer(encoder_output, src_mask)\n",
    "\n",
    "        # Decoder Pass\n",
    "        decoder_output = tgt_embedded\n",
    "        for layer in self.decoder:\n",
    "            # Pass the internally generated mask to the decoder layer\n",
    "            decoder_output = layer(decoder_output, encoder_output, src_mask, tgt_causal_mask)\n",
    "            \n",
    "        output = self.generator(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.561301Z",
     "iopub.status.busy": "2025-10-15T18:14:08.561040Z",
     "iopub.status.idle": "2025-10-15T18:14:08.580226Z",
     "shell.execute_reply": "2025-10-15T18:14:08.579603Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.561264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Create the Custom Dataset Class ---\n",
    "\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.X = self.df['X']\n",
    "        self.Y = self.df['Y']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X.iloc[idx], self.Y.iloc[idx]\n",
    "\n",
    "# --- 2. Define the Collate Function for Padding ---\n",
    "\n",
    "def collate_fn(batch, tokenizer, pad_token_id):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    bos_token_id = tokenizer.token_to_id('<bos>')\n",
    "    eos_token_id = tokenizer.token_to_id('<eos>')\n",
    "\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # Encode the strings and add BOS/EOS tokens\n",
    "        src_encoded = [bos_token_id] + tokenizer.encode(src_sample).ids + [eos_token_id]\n",
    "        tgt_encoded = [bos_token_id] + tokenizer.encode(tgt_sample).ids + [eos_token_id]\n",
    "        \n",
    "        src_batch.append(torch.tensor(src_encoded))\n",
    "        tgt_batch.append(torch.tensor(tgt_encoded))\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    # Our model expects (seq_len, batch_size)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=False, padding_value=pad_token_id)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=False, padding_value=pad_token_id)\n",
    "    \n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# --- 3. Instantiate the DataLoaders ---\n",
    "\n",
    "# Assuming 'train_df', 'val_df', and 'tokenizer' are already defined\n",
    "pad_id = tokenizer.token_to_id('<pad>')\n",
    "batch_size = 32 # Or 64 as per the spec\n",
    "\n",
    "train_dataset = ChatbotDataset(train_df, tokenizer)\n",
    "val_dataset = ChatbotDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=lambda batch: collate_fn(batch, tokenizer, pad_id)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=lambda batch: collate_fn(batch, tokenizer, pad_id)\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "# You can check a batch like this:\n",
    "# src_batch, tgt_batch = next(iter(train_loader))\n",
    "# print(\"Source batch shape:\", src_batch.shape)\n",
    "# print(\"Target batch shape:\", tgt_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:08.581075Z",
     "iopub.status.busy": "2025-10-15T18:14:08.580851Z",
     "iopub.status.idle": "2025-10-15T18:14:09.565916Z",
     "shell.execute_reply": "2025-10-15T18:14:09.565245Z",
     "shell.execute_reply.started": "2025-10-15T18:14:08.581060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready for training.\n",
      "\n",
      "INPUT: This was a best friend. I miss her.\n",
      "MODEL RESPONSE: budge jumping jumping jumping jumping jumping jumping jumping foot aint busiest :))) values render salespeople gates flash values render tales aint busiest youtube neckbeards spur narcissistic offguard huskies destroy rinsing talents growls visits rely seagull steroids unbearable winkler improtant taking persisted jumping etc :))) tottenham offguard huskies destroy bacteria\n"
     ]
    }
   ],
   "source": [
    "# --- 1. LOAD THE MODEL ---\n",
    "# --- SETUP AND INITIALIZATION FOR TRAINING ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pad_id = tokenizer.token_to_id('<pad>')\n",
    "\n",
    "# Initialize a NEW model for training\n",
    "print(\"Initializing a new model for training...\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = TransformerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    nhead=2,\n",
    "    d_hid=1024,\n",
    "    nlayers_encoder=2,\n",
    "    nlayers_decoder=2\n",
    ").to(device)\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98))\n",
    "\n",
    "print(\"Setup complete. Ready for training.\")\n",
    "\n",
    "# --- 2. GREEDY DECODING FUNCTION ---\n",
    "\n",
    "def generate_response(model, tokenizer, input_text, emotion, situation,\n",
    "                        max_len=50, device=\"cuda\"):\n",
    "    \n",
    "    # Preprocess and format the input string\n",
    "    normalized_situation = normalize_text(situation)\n",
    "    normalized_input = normalize_text(input_text)\n",
    "    prompt = (f\"Emotion: {emotion} | Situation: {normalized_situation} | \"\n",
    "              f\"Customer: {normalized_input} Agent:\")\n",
    "    \n",
    "    # Tokenize the source prompt\n",
    "    bos_token_id = tokenizer.token_to_id('<bos>')\n",
    "    eos_token_id = tokenizer.token_to_id('<eos>')\n",
    "    \n",
    "    src_tokens = [bos_token_id] + tokenizer.encode(prompt).ids + [eos_token_id]\n",
    "    src = torch.tensor(src_tokens).unsqueeze(1).to(device) # Shape: (seq_len, 1)\n",
    "\n",
    "    # Start the decoder output with the <bos> token\n",
    "    tgt_tokens = [bos_token_id]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len - 1):\n",
    "            tgt = torch.tensor(tgt_tokens).unsqueeze(1).to(device) # Shape: (tgt_len, 1)\n",
    "            \n",
    "            # Create masks for the model\n",
    "            tgt_seq_len = tgt.shape[0]\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
    "            \n",
    "            # Get model output (logits)\n",
    "            output = model(src=src, tgt=tgt, src_padding_mask=None, \n",
    "                           tgt_padding_mask=None, tgt_mask=tgt_mask)\n",
    "            \n",
    "            # Get the logits for the very last token\n",
    "            last_token_logits = output[-1, 0, :]\n",
    "            \n",
    "            # Find the token with the highest probability (greedy choice)\n",
    "            next_token_id = last_token_logits.argmax().item()\n",
    "            \n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt_tokens.append(next_token_id)\n",
    "            \n",
    "            # If the model predicts the end-of-sentence token, stop generating\n",
    "            if next_token_id == eos_token_id:\n",
    "                break\n",
    "                \n",
    "    # Decode the generated token IDs back to a string\n",
    "    generated_text = tokenizer.decode(tgt_tokens, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# --- 3. EXAMPLE USAGE ---\n",
    "\n",
    "# Let's try an example from your test set (or make one up)\n",
    "test_situation = \"I remember going to the fireworks with my best friend...\"\n",
    "test_emotion = \"sentimental\"\n",
    "test_utterance = \"This was a best friend. I miss her.\"\n",
    "\n",
    "response = generate_response(model, tokenizer, test_utterance, test_emotion, test_situation, device=device)\n",
    "\n",
    "print(f\"\\nINPUT: {test_utterance}\")\n",
    "print(f\"MODEL RESPONSE: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:09.566914Z",
     "iopub.status.busy": "2025-10-15T18:14:09.566657Z",
     "iopub.status.idle": "2025-10-15T18:14:10.493923Z",
     "shell.execute_reply": "2025-10-15T18:14:10.493190Z",
     "shell.execute_reply.started": "2025-10-15T18:14:09.566882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: This was a best friend. I miss her.\n",
      "--------------------\n",
      "GREEDY RESPONSE: budge jumping jumping jumping jumping jumping jumping jumping foot aint busiest :))) values render salespeople gates flash values render tales aint busiest youtube neckbeards spur narcissistic offguard huskies destroy rinsing talents growls visits rely seagull steroids unbearable winkler improtant taking persisted jumping etc :))) tottenham offguard huskies destroy bacteria\n",
      "BEAM SEARCH RESPONSE (k=5): budge jumping jumping jumping jumping jumping jumping jumping jumping jumping jumping jumping jumping foot button onesie attends seen lotions laugh np buddies lotions laugh steaming jumbled truly jumping successfull bead brazil values joyed month aunt frosting solitude boil jumping overfull sneaking offguard aint busiest colored acclaim confirmed jumping overfull\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTE: Ensure your trained model is already loaded and in eval mode.\n",
    "# model.load_state_dict(...)\n",
    "# model.eval()\n",
    "\n",
    "def beam_search_decode(model, tokenizer, input_text, emotion, situation,\n",
    "                         beam_width=5, max_len=50, device=\"cuda\"):\n",
    "    \n",
    "    # Preprocess and format the input string\n",
    "    normalized_situation = normalize_text(situation)\n",
    "    normalized_input = normalize_text(input_text)\n",
    "    prompt = (f\"Emotion: {emotion} | Situation: {normalized_situation} | \"\n",
    "              f\"Customer: {normalized_input} Agent:\")\n",
    "\n",
    "    # Tokenize the source prompt\n",
    "    bos_token_id = tokenizer.token_to_id('<bos>')\n",
    "    eos_token_id = tokenizer.token_to_id('<eos>')\n",
    "    src_tokens = [bos_token_id] + tokenizer.encode(prompt).ids + [eos_token_id]\n",
    "    src = torch.tensor(src_tokens).unsqueeze(1).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the encoder output once\n",
    "        src_emb = model.embedding(src) * math.sqrt(model.d_model)\n",
    "        src_emb = model.pos_encoder(src_emb)\n",
    "        encoder_output = model.transformer.encoder(src_emb)\n",
    "\n",
    "        # Start with the <bos> token\n",
    "        # Beams are stored as a list of tuples: (sequence, log_probability_score)\n",
    "        beams = [(torch.tensor([bos_token_id], device=device), 0.0)]\n",
    "        \n",
    "        for _ in range(max_len - 1):\n",
    "            candidates = []\n",
    "            for seq, score in beams:\n",
    "                # If a beam has ended, add it to candidates and continue\n",
    "                if seq[-1].item() == eos_token_id:\n",
    "                    candidates.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                # Get model predictions for the next token\n",
    "                tgt_input = seq.unsqueeze(1) # Shape: (current_len, 1)\n",
    "                tgt_emb = model.embedding(tgt_input) * math.sqrt(model.d_model)\n",
    "                tgt_emb = model.pos_encoder(tgt_emb)\n",
    "                \n",
    "                # Create masks\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(seq)).to(device)\n",
    "                \n",
    "                # Decoder forward pass\n",
    "                decoder_output = model.transformer.decoder(tgt_emb, encoder_output, tgt_mask=tgt_mask)\n",
    "                logits = model.generator(decoder_output)\n",
    "\n",
    "                # Get log probabilities for the last token\n",
    "                log_probs = F.log_softmax(logits[-1, 0, :], dim=-1)\n",
    "                \n",
    "                # Get the top k most likely next tokens\n",
    "                top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                # Create new beams\n",
    "                for i in range(beam_width):\n",
    "                    next_token = top_indices[i].unsqueeze(0)\n",
    "                    new_seq = torch.cat([seq, next_token], dim=0)\n",
    "                    new_score = score + top_log_probs[i].item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "\n",
    "            # Sort all candidates by score and select the top k\n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # If all top beams have ended, we can stop early\n",
    "            if all(b[0][-1].item() == eos_token_id for b in beams):\n",
    "                break\n",
    "    \n",
    "    # Choose the best beam (the one with the highest score)\n",
    "    best_seq, _ = beams[0]\n",
    "    generated_text = tokenizer.decode(best_seq.tolist(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "\n",
    "# Use the same example as before to see the improvement\n",
    "test_situation = \"I remember going to the fireworks with my best friend...\"\n",
    "test_emotion = \"sentimental\"\n",
    "test_utterance = \"This was a best friend. I miss her.\"\n",
    "\n",
    "# Get the greedy response again for comparison\n",
    "greedy_response = generate_response(model, tokenizer, test_utterance, test_emotion, test_situation, device=device)\n",
    "\n",
    "# Get the beam search response\n",
    "beam_response = beam_search_decode(model, tokenizer, test_utterance, test_emotion, test_situation, beam_width=5, device=device)\n",
    "\n",
    "print(f\"INPUT: {test_utterance}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"GREEDY RESPONSE: {greedy_response}\")\n",
    "print(f\"BEAM SEARCH RESPONSE (k=5): {beam_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:14:10.495057Z",
     "iopub.status.busy": "2025-10-15T18:14:10.494759Z",
     "iopub.status.idle": "2025-10-15T22:23:58.884744Z",
     "shell.execute_reply": "2025-10-15T22:23:58.884077Z",
     "shell.execute_reply.started": "2025-10-15T18:14:10.495030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Setup complete. Ready for training.\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1616 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.60it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [30:09<00:00,  3.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd686c6303c4da9869acf8cb1e284b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 4.6927 | Train Perplexity = 109.14 | Val BLEU = 0.57\n",
      "New best model saved with Val BLEU: 0.57\n",
      "\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.64it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [23:13<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 4.2220 | Train Perplexity = 68.17 | Val BLEU = 0.51\n",
      "\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.66it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [24:37<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 4.0918 | Train Perplexity = 59.85 | Val BLEU = 0.64\n",
      "New best model saved with Val BLEU: 0.64\n",
      "\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.55it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [30:18<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 4.0051 | Train Perplexity = 54.87 | Val BLEU = 0.93\n",
      "New best model saved with Val BLEU: 0.93\n",
      "\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.84it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [22:29<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 3.9417 | Train Perplexity = 51.51 | Val BLEU = 0.65\n",
      "\n",
      "--- Epoch 6/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.73it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [20:39<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 3.8909 | Train Perplexity = 48.95 | Val BLEU = 0.85\n",
      "\n",
      "--- Epoch 7/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.39it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [23:59<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 3.8492 | Train Perplexity = 46.96 | Val BLEU = 0.82\n",
      "\n",
      "--- Epoch 8/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.82it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [20:18<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 3.8148 | Train Perplexity = 45.37 | Val BLEU = 0.91\n",
      "\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:57<00:00, 27.92it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [21:58<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 3.7851 | Train Perplexity = 44.04 | Val BLEU = 0.90\n",
      "\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:58<00:00, 27.79it/s]\n",
      "Calculating Val BLEU: 100%|██████████| 6464/6464 [22:04<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 3.7556 | Train Perplexity = 42.76 | Val BLEU = 0.95\n",
      "New best model saved with Val BLEU: 0.95\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pad_id = tokenizer.token_to_id('<pad>')\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = ChatbotDataset(train_df, tokenizer)\n",
    "val_dataset = ChatbotDataset(val_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda b: collate_fn(b, tokenizer, pad_id))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda b: collate_fn(b, tokenizer, pad_id))\n",
    "\n",
    "# Initialize the Model for a SINGLE GPU\n",
    "print(f\"Using device: {device}\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = TransformerModel(vocab_size=vocab_size, d_model=256, nhead=2, d_hid=1024,\n",
    "                         nlayers_encoder=2, nlayers_decoder=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98))\n",
    "\n",
    "print(\"Setup complete. Ready for training.\")\n",
    "\n",
    "# --- 4. TRAINING & VALIDATION LOOPS ---\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, pad_token_id):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in tqdm(loader, desc=\"Training\"):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_output = tgt[1:, :]\n",
    "\n",
    "        # Create masks for nn.Transformer\n",
    "        tgt_seq_len = tgt_input.shape[0]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
    "        src_padding_mask = (src == pad_token_id).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt_input == pad_token_id).transpose(0, 1)\n",
    "\n",
    "        output = model(src, tgt_input, src_padding_mask, tgt_padding_mask, tgt_mask)\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_and_get_bleu(model, device, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Loop through the validation dataframe to generate predictions\n",
    "        for index, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Calculating Val BLEU\"):\n",
    "            input_text = row['empathetic_dialogues']\n",
    "            emotion = row['emotion']\n",
    "            situation = row['Situation']\n",
    "            ground_truth = row['labels']\n",
    "            \n",
    "            model_output = beam_search_decode(model, tokenizer, input_text, emotion, situation)\n",
    "            \n",
    "            predictions.append(model_output)\n",
    "            references.append(ground_truth)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # The 'references' argument should be a list of lists,\n",
    "    # where each inner list contains one reference sentence.\n",
    "    bleu_results = sacrebleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    \n",
    "    return bleu_results['score']\n",
    "\n",
    "\n",
    "# --- UPDATED MAIN TRAINING DRIVER ---\n",
    "NUM_EPOCHS = 10\n",
    "best_val_bleu = 0.0  # Initialize with 0.0 for BLEU\n",
    "model_save_path = '/kaggle/working/best_chatbot_model.pth'\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Training loop remains the same\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, pad_id)\n",
    "    \n",
    "    # New validation step\n",
    "    val_bleu = validate_and_get_bleu(model, device, tokenizer)\n",
    "    \n",
    "    # Calculate perplexity from training loss\n",
    "    train_perplexity = math.exp(train_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f} | Train Perplexity = {train_perplexity:.2f} | Val BLEU = {val_bleu:.2f}\")\n",
    "    \n",
    "    # Save the model if validation BLEU has improved\n",
    "    if val_bleu > best_val_bleu:\n",
    "        best_val_bleu = val_bleu\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved with Val BLEU: {val_bleu:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T22:23:58.885875Z",
     "iopub.status.busy": "2025-10-15T22:23:58.885597Z",
     "iopub.status.idle": "2025-10-15T22:46:01.041366Z",
     "shell.execute_reply": "2025-10-15T22:46:01.040594Z",
     "shell.execute_reply.started": "2025-10-15T22:23:58.885850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6464/6464 [22:02<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Loop through your test dataframe\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    input_text = row['empathetic_dialogues']\n",
    "    emotion = row['emotion']\n",
    "    situation = row['Situation']\n",
    "    ground_truth = row['labels']\n",
    "    \n",
    "    # Generate a response using your best decoding method\n",
    "    model_output = beam_search_decode(model, tokenizer, input_text, emotion, situation)\n",
    "    \n",
    "    predictions.append(model_output)\n",
    "    references.append(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T22:46:01.042541Z",
     "iopub.status.busy": "2025-10-15T22:46:01.042218Z",
     "iopub.status.idle": "2025-10-15T22:46:03.440959Z",
     "shell.execute_reply": "2025-10-15T22:46:03.439705Z",
     "shell.execute_reply.started": "2025-10-15T22:46:01.042514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01dd781d75a44cf8495443eaa0ddfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "validate_params() got an unexpected keyword argument 'prefer_skip_nested_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/2998380401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load all the required metric modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msacrebleu_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sacrebleu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrouge_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mchrf_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chrf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mSpecific\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mdownload_mode\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mDownload\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgenerate\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0mrevision\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0mrepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m             raise FileNotFoundError(\n\u001b[1;32m    682\u001b[0m                 \u001b[0;34mf\"Couldn't find a module script at {relative_to_absolute_path(combined_path)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                 \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                                 \u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m                             ).get_module()\n\u001b[0m\u001b[1;32m    640\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mimports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_imports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         local_imports = _download_additional_modules(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_hub_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary_import_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa F841\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mlibrary_import_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"scikit-learn\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlibrary_import_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sklearn\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlibrary_import_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mneeds_to_be_installed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary_import_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_import_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m \"\"\"\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_entity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxent_NE_Chunker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpChunkParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/parse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshiftreduce\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShiftReduceParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSteppingShiftReduceParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitionparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransitionParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTestGrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_test_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviterbi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViterbiParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/parse/transitionparser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# SPDX-License-Identifier: BSD-3-Clause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bounds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml1_min_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNuSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNuSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneClassSVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_bounds.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m @validate_params(\n\u001b[0m\u001b[1;32m     17\u001b[0m     {\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m\"X\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"array-like\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sparse matrix\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: validate_params() got an unexpected keyword argument 'prefer_skip_nested_validation'"
     ]
    }
   ],
   "source": [
    "# Load all the required metric modules\n",
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "# Calculate the scores\n",
    "bleu_results = sacrebleu_metric.compute(predictions=predictions, references=references)\n",
    "rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "chrf_results = chrf_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"\\n--- Automatic Metric Results ---\")\n",
    "print(f\"BLEU: {bleu_results['score']:.2f}\")\n",
    "print(f\"ROUGE-L: {rouge_results['rougeL'] * 100:.2f}\")\n",
    "print(f\"chrF: {chrf_results['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Human Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-15T22:46:03.441412Z",
     "iopub.status.idle": "2025-10-15T22:46:03.441626Z",
     "shell.execute_reply": "2025-10-15T22:46:03.441529Z",
     "shell.execute_reply.started": "2025-10-15T22:46:03.441519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define how many samples you want to evaluate\n",
    "NUM_SAMPLES_TO_EVALUATE = 25\n",
    "\n",
    "# Get the original inputs from your test dataframe\n",
    "inputs = test_df['X'].head(NUM_SAMPLES_TO_EVALUATE).tolist()\n",
    "\n",
    "# Get the corresponding ground truth and model predictions\n",
    "ground_truth_replies = references[:NUM_SAMPLES_TO_EVALUATE]\n",
    "model_generated_replies = predictions[:NUM_SAMPLES_TO_EVALUATE]\n",
    "\n",
    "# Create a new DataFrame for easy viewing and scoring\n",
    "evaluation_df = pd.DataFrame({\n",
    "    'Input_Prompt': inputs,\n",
    "    'Ground_Truth_Reply': ground_truth_replies,\n",
    "    'Model_Generated_Reply': model_generated_replies,\n",
    "    'Fluency (1-5)': '',    # Empty column for you to fill in\n",
    "    'Relevance (1-5)': '',   # Empty column for you to fill in\n",
    "    'Adequacy (1-5)': ''     # Empty column for you to fill in\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "evaluation_filename = '/kaggle/working/human_evaluation_sheet.csv'\n",
    "evaluation_df.to_csv(evaluation_filename, index=False)\n",
    "\n",
    "print(f\"Human evaluation sheet saved to '{evaluation_filename}'\")\n",
    "print(\"Here are the first 5 rows to review:\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "display(evaluation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1962861,
     "isSourceIdPinned": false,
     "sourceId": 3238154,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

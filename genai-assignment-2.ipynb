{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":3238154,"datasetId":1962861,"databundleVersionId":3288302,"isSourceIdPinned":false}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Libraries Import**","metadata":{}},{"cell_type":"code","source":"import kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:15.049927Z","iopub.execute_input":"2025-10-13T10:15:15.050524Z","iopub.status.idle":"2025-10-13T10:15:15.055097Z","shell.execute_reply.started":"2025-10-13T10:15:15.050499Z","shell.execute_reply":"2025-10-13T10:15:15.054250Z"}},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":"## **Task 1**","metadata":{}},{"cell_type":"code","source":"# Download latest version\npath = kagglehub.dataset_download(\"atharvjairath/empathetic-dialogues-facebook-ai\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:15.056362Z","iopub.execute_input":"2025-10-13T10:15:15.056644Z","iopub.status.idle":"2025-10-13T10:15:15.590011Z","shell.execute_reply.started":"2025-10-13T10:15:15.056625Z","shell.execute_reply":"2025-10-13T10:15:15.589110Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/empathetic-dialogues-facebook-ai\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"# The path provided by kagglehub\ndataset_path = \"/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\"\n\n# Load the dataset into a pandas DataFrame\ndf = pd.read_csv(dataset_path)\nprint(f\"Dataset loaded successfully. Total rows: {len(df)}\")\n\n# First, split into 80% train and 20% temporary (for val + test)\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Next, split the 20% temporary set in half to get 10% validation and 10% test\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\nprint(f\"Training set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(val_df)}\")\nprint(f\"Test set size: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:15.591487Z","iopub.execute_input":"2025-10-13T10:15:15.591889Z","iopub.status.idle":"2025-10-13T10:15:15.912284Z","shell.execute_reply.started":"2025-10-13T10:15:15.591868Z","shell.execute_reply":"2025-10-13T10:15:15.911540Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully. Total rows: 64636\nTraining set size: 51708\nValidation set size: 6464\nTest set size: 6464\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"def normalize_text(text):\n    text = str(text).lower()\n    \n    # Rule 1: Remove \"customer :\" or \"agent :\" from the BEGINNING of the string\n    text = re.sub(r'^(customer|agent)\\s*:\\s*', '', text, flags=re.I).strip()\n\n    # Rule 2: Remove \"agent :\" from the END of the string\n    text = re.sub(r'agent\\s*:\\s*$', '', text, flags=re.I).strip()\n\n    # Rule 3: Handle punctuation and whitespace\n    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:15.913117Z","iopub.execute_input":"2025-10-13T10:15:15.913368Z","iopub.status.idle":"2025-10-13T10:15:15.918925Z","shell.execute_reply.started":"2025-10-13T10:15:15.913349Z","shell.execute_reply":"2025-10-13T10:15:15.917873Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"# Apply the function to the correct columns in all three data splits\nfor df_split in [train_df, val_df, test_df]:\n    df_split['Situation_normalized'] = df_split['Situation'].apply(normalize_text)\n    df_split['empathetic_dialogues_normalized'] = df_split['empathetic_dialogues'].apply(normalize_text)\n    df_split['labels_normalized'] = df_split['labels'].apply(normalize_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:15.920999Z","iopub.execute_input":"2025-10-13T10:15:15.921242Z","iopub.status.idle":"2025-10-13T10:15:18.681476Z","shell.execute_reply.started":"2025-10-13T10:15:15.921223Z","shell.execute_reply":"2025-10-13T10:15:18.680769Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"# Create a list of all text from the training set to build the vocabulary\ncorpus = list(train_df['empathetic_dialogues_normalized']) + \\\n         list(train_df['Situation_normalized']) + \\\n         list(train_df['labels_normalized'])\n\n# We need an iterator for the tokenizer training\ndef corpus_iterator():\n    for text in corpus:\n        yield text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:18.682468Z","iopub.execute_input":"2025-10-13T10:15:18.682800Z","iopub.status.idle":"2025-10-13T10:15:18.722252Z","shell.execute_reply.started":"2025-10-13T10:15:18.682767Z","shell.execute_reply":"2025-10-13T10:15:18.721218Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# Initialize a tokenizer\ntokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\ntokenizer.pre_tokenizer = Whitespace()\n\n# Define the trainer\ntrainer = WordLevelTrainer(special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n\n# Train the tokenizer on your training data\ntokenizer.train_from_iterator(corpus_iterator(), trainer=trainer)\n\nvocab_size = tokenizer.get_vocab_size()\nprint(f\"Vocabulary size: {vocab_size}\")\n\n# Save the tokenizer for later use \nsave_path = \"/kaggle/working/my_tokenizer.json\"\ntokenizer.save(save_path)\nprint(f\"Tokenizer saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:18.723474Z","iopub.execute_input":"2025-10-13T10:15:18.723804Z","iopub.status.idle":"2025-10-13T10:15:20.084347Z","shell.execute_reply.started":"2025-10-13T10:15:18.723781Z","shell.execute_reply":"2025-10-13T10:15:20.083490Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 19342\nTokenizer saved to /kaggle/working/my_tokenizer.json\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"# tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\n\nprint(\"Special token IDs:\")\nprint(f\"<unk> ID: {tokenizer.token_to_id('<unk>')}\")\nprint(f\"<pad> ID: {tokenizer.token_to_id('<pad>')}\")\nprint(f\"<bos> ID: {tokenizer.token_to_id('<bos>')}\")\nprint(f\"<eos> ID: {tokenizer.token_to_id('<eos>')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:20.085313Z","iopub.execute_input":"2025-10-13T10:15:20.085627Z","iopub.status.idle":"2025-10-13T10:15:20.090813Z","shell.execute_reply.started":"2025-10-13T10:15:20.085595Z","shell.execute_reply":"2025-10-13T10:15:20.090082Z"}},"outputs":[{"name":"stdout","text":"Special token IDs:\n<unk> ID: 3\n<pad> ID: 0\n<bos> ID: 1\n<eos> ID: 2\n","output_type":"stream"}],"execution_count":111},{"cell_type":"markdown","source":"## **Task 2**","metadata":{}},{"cell_type":"code","source":"def create_input_string(row):\n    emotion = row['emotion']\n    situation = row['Situation_normalized']\n    customer_utterance = row['empathetic_dialogues_normalized']\n    \n    # Format the string exactly as specified\n    input_str = f\"Emotion: {emotion} | Situation: {situation} | Customer: {customer_utterance} Agent:\"\n    return input_str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:20.091601Z","iopub.execute_input":"2025-10-13T10:15:20.091868Z","iopub.status.idle":"2025-10-13T10:15:20.106107Z","shell.execute_reply.started":"2025-10-13T10:15:20.091840Z","shell.execute_reply":"2025-10-13T10:15:20.105224Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"# Apply the function to create the 'X' and 'Y' columns for all data splits\nfor df_split in [train_df, val_df, test_df]:\n    df_split['X'] = df_split.apply(create_input_string, axis=1)\n    # The target 'Y' is the normalized agent's reply from the 'labels' column\n    df_split['Y'] = df_split['labels_normalized']\n\nprint(\"X and Y columns created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:20.106913Z","iopub.execute_input":"2025-10-13T10:15:20.107179Z","iopub.status.idle":"2025-10-13T10:15:20.750245Z","shell.execute_reply.started":"2025-10-13T10:15:20.107143Z","shell.execute_reply":"2025-10-13T10:15:20.749482Z"}},"outputs":[{"name":"stdout","text":"X and Y columns created successfully.\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"# Inspect a final example from the training set to verify\nprint(\"\\n--- Example ---\")\nprint(\"INPUT (X):\")\nprint(train_df['X'].iloc[0])\nprint(\"\\nTARGET (Y):\")\nprint(train_df['Y'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T10:15:20.752169Z","iopub.execute_input":"2025-10-13T10:15:20.752737Z","iopub.status.idle":"2025-10-13T10:15:20.757884Z","shell.execute_reply.started":"2025-10-13T10:15:20.752707Z","shell.execute_reply":"2025-10-13T10:15:20.757133Z"}},"outputs":[{"name":"stdout","text":"\n--- Example ---\nINPUT (X):\nEmotion: nostalgic | Situation: i had to go buy legos for my nephew the other day . makes me miss the days when my girls were young enough to play with them . | Customer: were you embarrassed or what happend ? Agent:\n\nTARGET (Y):\nno just this feeling overcame me that my kids just have outgrown this time .\n","output_type":"stream"}],"execution_count":114}]}